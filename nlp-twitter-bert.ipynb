{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b7cb1a15fc4103e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "import string\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c00a81e9ca5d235"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af9d99eb3e2c52cf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "test_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "# Set the maximum column width to a large number for all columns\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.reset_option('display.max_colwidth')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca8ad1c2ff4375e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset preprocessing\n",
    "\n",
    "1. Removing emojis, urls, usernames, duplicates, NaN values, hashtags\n",
    "2. Lowercasing characters\n",
    "3. Joining hashtags and keywords nad text together"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc721eac0627be73"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Preproceanyssing function\n",
    "def preprocess_text(df):\n",
    "    # Make dataset lowercase\n",
    "    df[\"text\"] = df[\"text\"].str.lower()\n",
    "\n",
    "    # Create new table to pass their hashtags\n",
    "    df[\"hashtags\"] = df[\"text\"].apply(extract_hashtags)\n",
    "\n",
    "    # Remove hashtags\n",
    "    df[\"text\"] = df[\"text\"].apply(remove_hashtags)\n",
    "\n",
    "    # Remove url links\n",
    "    df[\"text\"] = df[\"text\"].apply(remove_url_username)\n",
    "\n",
    "    # Remove emojis\n",
    "    df[\"text\"] = df[\"text\"].apply(remove_emojis)\n",
    "\n",
    "    # Remove special characters\n",
    "    df['text'] = df['text'].apply(remove_special_characters)\n",
    "\n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates(subset=['text'], keep='first')\n",
    "\n",
    "    df[\"combined_text\"] = df.apply(append_hashtags_text, axis=1)\n",
    "\n",
    "    # Remove NaN values from keywords\n",
    "    df['keyword'] = data['keyword'].fillna('')\n",
    "\n",
    "    # Concatenate combined_text and keyword\n",
    "    df['final_text'] = np.where(\n",
    "        df['keyword'].str.strip() != '',\n",
    "        \"Keyword: \" + df['keyword'] + \" \" + df['combined_text'],\n",
    "        df['combined_text']\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Extract hashtags\n",
    "def extract_hashtags(text):\n",
    "    return re.findall(r\"#(\\w+)\", text)\n",
    "\n",
    "# Function to remove hashtags from text\n",
    "def remove_hashtags(text):\n",
    "    return re.sub(r\"#(\\w+)\", \"\", text).strip()\n",
    "\n",
    "def remove_url_username(text):\n",
    "    url_pattern = r\"http[s]?://\\S+\"\n",
    "    return re.sub(url_pattern, \"\", text).strip()\n",
    "\n",
    "# Remove emojis from text\n",
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, \"\").strip()\n",
    "\n",
    "# Remove special characters\n",
    "def remove_special_characters(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Append hashtags to text\n",
    "def append_hashtags_text(row):\n",
    "    hashtags = \", \".join(row['hashtags'])  # Join list with spaces\n",
    "    text = row['text'].strip()\n",
    "    if hashtags:\n",
    "        return f\"Hashtags: {hashtags}. Text: {row['text']}\"\n",
    "    else:\n",
    "        return f\"Text: {text}\"\n",
    "\n",
    "\n",
    "data = preprocess_text(data)\n",
    "test_data = preprocess_text(test_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4382049fa5b3b2b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Splitting data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fd4e723779e4345"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data[['final_text', 'target']].dropna()\n",
    "\n",
    "# Split the data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    data['final_text'].tolist(),\n",
    "    data['target'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_data = test_data[['id', 'final_text']].dropna()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d5f3740af955e69"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['final_text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Save it as a Dataset to load then into the model\n",
    "train_dataset = Dataset.from_dict({'text': train_texts, 'label': train_labels}).map(tokenize_function, batched=True)\n",
    "val_dataset = Dataset.from_dict({'text': val_texts, 'label': val_labels}).map(tokenize_function, batched=True)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5b67f00f3e50ec8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Deactivate weights & biases\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24ab22c5986ec8a5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83b0ff185bccc64b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbf92ee937a8d951"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Evaluate the model on validation set\n",
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2351147df34a1cb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# We need convert ids to string to put on a table\n",
    "test_data['id'] = test_data['id'].astype(str)\n",
    "\n",
    "# Taking final_text and id\n",
    "test_texts = test_data['final_text'].tolist()\n",
    "test_ids = test_data['id'].tolist()\n",
    "\n",
    "# Tokenize the test data\n",
    "test_dataset = Dataset.from_dict({'id': test_ids, 'text': test_texts}).map(tokenize_function, batched=True)\n",
    "\n",
    "# Get predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "predicted_labels = predictions.predictions.argmax(-1)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'target': predicted_labels,\n",
    "})\n",
    "\n",
    "print(results_df.head())\n",
    "\n",
    "results_df.to_csv('predictions_sent_first_s.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "324a6fdd346b57f2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
