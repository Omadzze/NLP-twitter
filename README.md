# Solution for Natural Language Processing with Disaster Tweets

Notebook [nlp-twitter-bert.ipynb](https://github.com/Omadzze/NLP-twitter/blob/main/nlp-twitter-bert.ipynb) contains solution which were used BERT model for text-classifciation. First, the data preprocessing process was done and all the unecessary data like: links, emojis, usernames and etc. was deleted. We then started tokenizing data and preparing it for fine-tuning.

Moreover, [nlp-twitter-gemma.ipynb](https://github.com/Omadzze/NLP-twitter/blob/main/nlp-twitter-gemma.ipynb) this contains solution for the Gemma model which performing better than BERT model.

Gemma model score: 0.82899
BERT model score: 0.82163
