{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## HuggingFace\n","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_api_token = user_secrets.get_secret(\"huggingfaceAPI\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token=hf_api_token)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Installation","metadata":{}},{"cell_type":"code","source":"!pip install peft==0.8.2\n!pip install bitsandbytes==0.42.0\n!pip install accelerate==0.26.1\n!pip install datasets==2.16.1\n!pip install GPUtil\n!pip install transformers==4.38.0\n!pip install peft==0.8.2\n!pip install bitsandbytes==0.42.0\n!pip install accelerate==0.26.1\n!pip install datasets==2.16.1\n!pip install evaluate\n\npip install emoji --upgrade","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport emoji\nimport string\nimport torch\nfrom transformers Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nfrom textblob import TextBlob\nfrom datasets import Dataset","metadata":{"execution":{"iopub.status.busy":"2024-08-08T07:43:40.060096Z","iopub.execute_input":"2024-08-08T07:43:40.060438Z","iopub.status.idle":"2024-08-08T07:43:41.248892Z","shell.execute_reply.started":"2024-08-08T07:43:40.060404Z","shell.execute_reply":"2024-08-08T07:43:41.247835Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n# Set the maximum column width to a large number for all columns\npd.set_option('display.max_colwidth', None)\npd.reset_option('display.max_colwidth')","metadata":{"execution":{"iopub.status.busy":"2024-08-08T07:43:41.250043Z","iopub.execute_input":"2024-08-08T07:43:41.250614Z","iopub.status.idle":"2024-08-08T07:43:41.309899Z","shell.execute_reply.started":"2024-08-08T07:43:41.250572Z","shell.execute_reply":"2024-08-08T07:43:41.309130Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Dataset preprocessing\n\n1. Removing emojis, urls, usernames, duplicates, NaN values, hashtags\n2. Lowercasing characters\n3. Joining hashtags and keywords nad text together","metadata":{}},{"cell_type":"code","source":"# Preproceanyssing function\ndef preprocess_text(df):\n    # Make dataset lowercase\n    df[\"text\"] = df[\"text\"].str.lower()\n    \n    # Create new table to pass their hashtags\n    df[\"hashtags\"] = df[\"text\"].apply(extract_hashtags)\n    \n    # Remove hashtags\n    df[\"text\"] = df[\"text\"].apply(remove_hashtags)\n    \n    # Remove url links\n    df[\"text\"] = df[\"text\"].apply(remove_url_username)\n    \n    # Remove emojis\n    df[\"text\"] = df[\"text\"].apply(remove_emojis)\n    \n    # Remove special characters\n    df['text'] = df['text'].apply(remove_special_characters)\n    \n    # Remove duplicates\n    df = df.drop_duplicates(subset=['text'], keep='first')\n    \n    df[\"combined_text\"] = df.apply(append_hashtags_text, axis=1)\n    \n    # Remove NaN values from keywords\n    df['keyword'] = data['keyword'].fillna('')\n    \n    # Concatenate combined_text and keyword\n    df['final_text'] = np.where(\n        df['keyword'].str.strip() != '',\n        \"Keyword: \" + df['keyword'] + \" \" + df['combined_text'],\n        df['combined_text']\n    )\n    \n    return df\n\n\n# Extract hashtags\ndef extract_hashtags(text):\n    return re.findall(r\"#(\\w+)\", text)\n\n# Function to remove hashtags from text\ndef remove_hashtags(text):\n    return re.sub(r\"#(\\w+)\", \"\", text).strip()\n\ndef remove_url_username(text):\n    url_pattern = r\"http[s]?://\\S+\"\n    return re.sub(url_pattern, \"\", text).strip()\n\n# Remove emojis from text\ndef remove_emojis(text):\n    return emoji.replace_emoji(text, \"\").strip()\n\n# Remove special characters\ndef remove_special_characters(text):\n    return text.translate(str.maketrans('', '', string.punctuation))\n\n# Append hashtags to text\ndef append_hashtags_text(row):\n    hashtags = \", \".join(row['hashtags'])  # Join list with spaces\n    text = row['text'].strip()\n    if hashtags:\n        return f\"Hashtags: {hashtags}. Text: {row['text']}\"\n    else:\n        return f\"Text: {text}\"\n\n\ndata = preprocess_text(data)\ntest_data = preprocess_text(test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into training and validation sets\ntrain_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n\n# Display the size of each dataset\nprint(f\"Training data size: {train_data.shape[0]}\")\nprint(f\"Validation data size: {val_data.shape[0]}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-08T07:44:11.764617Z","iopub.execute_input":"2024-08-08T07:44:11.765206Z","iopub.status.idle":"2024-08-08T07:44:11.776486Z","shell.execute_reply.started":"2024-08-08T07:44:11.765166Z","shell.execute_reply":"2024-08-08T07:44:11.775417Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Training data size: 5496\nValidation data size: 1375\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## DATA CHECK","metadata":{}},{"cell_type":"code","source":"# Drop uneccessary columns and keep only this\ntrain_data = train_data[['final_text', 'target']].dropna()\nval_data = val_data[['final_text', 'target']].dropna()","metadata":{"execution":{"iopub.status.busy":"2024-08-08T07:45:29.097731Z","iopub.execute_input":"2024-08-08T07:45:29.098077Z","iopub.status.idle":"2024-08-08T07:45:29.108914Z","shell.execute_reply.started":"2024-08-08T07:45:29.098051Z","shell.execute_reply":"2024-08-08T07:45:29.107452Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Rename columns to \"label\", then we will pass it LoRA\ntrain_data = train_data.rename(columns={'target': \"label\"})\nval_data = val_data.rename(columns={'target': \"label\"})","metadata":{"execution":{"iopub.status.busy":"2024-08-08T07:45:29.258963Z","iopub.execute_input":"2024-08-08T07:45:29.259769Z","iopub.status.idle":"2024-08-08T07:45:29.266363Z","shell.execute_reply.started":"2024-08-08T07:45:29.259724Z","shell.execute_reply":"2024-08-08T07:45:29.265239Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"test_data.info()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T11:38:23.729233Z","iopub.execute_input":"2024-08-07T11:38:23.729861Z","iopub.status.idle":"2024-08-07T11:38:23.739525Z","shell.execute_reply.started":"2024-08-07T11:38:23.729829Z","shell.execute_reply":"2024-08-07T11:38:23.738594Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3263 entries, 0 to 3262\nData columns (total 2 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   id          3263 non-null   int64 \n 1   final_text  3263 non-null   object\ndtypes: int64(1), object(1)\nmemory usage: 51.1+ KB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"MODEL_ID = \"google/gemma-2b-it\"\nBATCH_SIZE = 8\nEPOCHS = 4","metadata":{"execution":{"iopub.status.busy":"2024-08-08T07:56:16.338313Z","iopub.execute_input":"2024-08-08T07:56:16.339187Z","iopub.status.idle":"2024-08-08T07:56:16.343235Z","shell.execute_reply.started":"2024-08-08T07:56:16.339150Z","shell.execute_reply":"2024-08-08T07:56:16.342296Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Setup tokenizer\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=hf_api_token)\n\nprint(tokenizer.padding_side, tokenizer.pad_token)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Keep data as a dataset for data preprocessing in LLM\nfrom datasets import Dataset, DatasetDict\n\ntrain_data = Dataset.from_pandas(train_data)\n\nval_data = Dataset.from_pandas(val_data)\n\ndataset = DatasetDict({\n    'train': train_data,\n    'test': val_data\n})","metadata":{"execution":{"iopub.status.busy":"2024-08-08T07:47:58.472790Z","iopub.execute_input":"2024-08-08T07:47:58.473170Z","iopub.status.idle":"2024-08-08T07:47:59.126059Z","shell.execute_reply.started":"2024-08-08T07:47:58.473133Z","shell.execute_reply":"2024-08-08T07:47:59.125087Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Divide data to the train and test and keep in Dataset\n\ntokenized_dataset = {}\n\nfor split in dataset.keys():\n    tokenized_dataset[split] = dataset[split].map(\n        lambda x: tokenizer(x[\"final_text\"], truncation = True), batched = True)\n\ntokenized_dataset[\"train\"], tokenized_dataset[\"test\"]","metadata":{"execution":{"iopub.status.busy":"2024-08-08T07:48:12.932074Z","iopub.execute_input":"2024-08-08T07:48:12.933390Z","iopub.status.idle":"2024-08-08T07:48:14.256017Z","shell.execute_reply.started":"2024-08-08T07:48:12.933344Z","shell.execute_reply":"2024-08-08T07:48:14.255118Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5496 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09a49dccff2f4f1fb3141be34c9bfbec"}},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1375 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f829086245248b5b2e7289979e49fbf"}},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"(Dataset({\n     features: ['final_text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n     num_rows: 5496\n }),\n Dataset({\n     features: ['final_text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n     num_rows: 1375\n }))"},"metadata":{}}]},{"cell_type":"markdown","source":"# Gemma 2B","metadata":{}},{"cell_type":"markdown","source":"## Testing data","metadata":{}},{"cell_type":"code","source":"# Label the data. LLM will output answers using this labels\nid2label = {0: \"NOT HAZARDOUS\", 1: \"HAZARDOUS\"}\nlabel2id = {\"NOT HAZARDOUS\": 0, \"HAZARDOUS\": 1}","metadata":{"execution":{"iopub.status.busy":"2024-08-08T07:48:21.244699Z","iopub.execute_input":"2024-08-08T07:48:21.245338Z","iopub.status.idle":"2024-08-08T07:48:21.249889Z","shell.execute_reply.started":"2024-08-08T07:48:21.245305Z","shell.execute_reply":"2024-08-08T07:48:21.248965Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### Loading model to the GPU","metadata":{}},{"cell_type":"code","source":"# Load the model\n\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_ID,\n    num_labels = 2, # sentiment classification: true or false\n    id2label = id2label,\n    label2id = label2id, \n    load_in_8bit = True, # load 8-bit quantized model\n    token=hf_api_token # use huggingface API to load the model)\n\nprint(model.config.pad_token_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T07:48:52.450859Z","iopub.execute_input":"2024-08-08T07:48:52.451766Z","iopub.status.idle":"2024-08-08T07:48:52.458001Z","shell.execute_reply.started":"2024-08-08T07:48:52.451729Z","shell.execute_reply":"2024-08-08T07:48:52.457086Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"GemmaForSequenceClassification(\n  (model): GemmaModel(\n    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n    (layers): ModuleList(\n      (0-17): 18 x GemmaDecoderLayer(\n        (self_attn): GemmaSdpaAttention(\n          (q_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear8bitLt(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear8bitLt(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear8bitLt(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear8bitLt(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear8bitLt(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n    )\n    (norm): GemmaRMSNorm()\n  )\n  (score): Linear(in_features=2048, out_features=2, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Vanilla model to LoRA model\n\nfrom peft import prepare_model_for_int8_training\n\nmodel = prepare_model_for_int8_training(model)\n\nmodel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, TaskType, get_peft_model\n\nlora_config = LoraConfig(\n    r = 64,\n    lora_alpha = 32,\n    lora_dropout = 0.1,\n    task_type = TaskType.SEQ_CLS,\n    target_modules = \"all-linear\"\n)\n\nlora_config","metadata":{"execution":{"iopub.status.busy":"2024-08-08T07:48:55.720822Z","iopub.execute_input":"2024-08-08T07:48:55.721481Z","iopub.status.idle":"2024-08-08T07:48:55.729088Z","shell.execute_reply.started":"2024-08-08T07:48:55.721452Z","shell.execute_reply":"2024-08-08T07:48:55.728086Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.SEQ_CLS: 'SEQ_CLS'>, inference_mode=False, r=64, target_modules='all-linear', lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={})"},"metadata":{}}]},{"cell_type":"code","source":"lora_model = get_peft_model(model, lora_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will fine-tune model on 3 perecent of weights\nlora_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-08-08T07:48:56.948846Z","iopub.execute_input":"2024-08-08T07:48:56.949119Z","iopub.status.idle":"2024-08-08T07:48:56.958547Z","shell.execute_reply.started":"2024-08-08T07:48:56.949094Z","shell.execute_reply":"2024-08-08T07:48:56.957641Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"trainable params: 78,581,888 || all params: 2,584,889,600 || trainable%: 3.040048132036277\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Train the model","metadata":{}},{"cell_type":"code","source":"# Early stopping technqiue for training our model\n\nfrom transformers import EarlyStoppingCallback\n\nearly_stop = EarlyStoppingCallback(early_stopping_patience=1, early_stopping_threshold=.0)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T07:49:16.885503Z","iopub.execute_input":"2024-08-08T07:49:16.886419Z","iopub.status.idle":"2024-08-08T07:49:16.914572Z","shell.execute_reply.started":"2024-08-08T07:49:16.886370Z","shell.execute_reply":"2024-08-08T07:49:16.913814Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"#import evaluate\nfrom transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n\n# We will compute the metric of accuracy\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis = 1)\n    return {\"accuracy\": (predictions == labels).mean()}\n\n# This will train our model\ntrainer = Trainer(\n    model = lora_model,\n    args = TrainingArguments(\n        output_dir = \"./data/\",\n        learning_rate= 2e-5,\n        per_device_train_batch_size = BATCH_SIZE,\n        per_device_eval_batch_size = BATCH_SIZE,\n        evaluation_strategy = \"epoch\",\n        save_strategy = \"epoch\",\n        num_train_epochs = EPOCHS,\n        weight_decay = 0.01,\n        load_best_model_at_end = True,\n        logging_steps = 10,\n        report_to = \"none\"\n    ),\n    \n    train_dataset = tokenized_dataset[\"train\"],\n    eval_dataset = tokenized_dataset[\"test\"],\n    tokenizer = tokenizer,\n    data_collator = DataCollatorWithPadding(tokenizer = tokenizer),\n    compute_metrics = compute_metrics,\n    #callbacks=[early_stop]  # early stopping callback\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T07:59:02.861714Z","iopub.execute_input":"2024-08-08T07:59:02.862342Z","iopub.status.idle":"2024-08-08T07:59:05.191770Z","shell.execute_reply.started":"2024-08-08T07:59:02.862307Z","shell.execute_reply":"2024-08-08T07:59:05.191017Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"print(\"Train dataset size:\", len(tokenized_dataset[\"train\"]))\nprint(\"Test dataset size:\", len(tokenized_dataset[\"test\"]))","metadata":{"execution":{"iopub.status.busy":"2024-08-08T07:59:05.763483Z","iopub.execute_input":"2024-08-08T07:59:05.764149Z","iopub.status.idle":"2024-08-08T07:59:05.769301Z","shell.execute_reply.started":"2024-08-08T07:59:05.764113Z","shell.execute_reply":"2024-08-08T07:59:05.768317Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Train dataset size: 5496\nTest dataset size: 1375\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-08-08T07:59:07.358359Z","iopub.execute_input":"2024-08-08T07:59:07.358721Z","iopub.status.idle":"2024-08-08T07:59:07.367781Z","shell.execute_reply.started":"2024-08-08T07:59:07.358687Z","shell.execute_reply":"2024-08-08T07:59:07.366639Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"print(\"Evaluating the model before training!\")\ntrainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-08-08T07:51:56.098033Z","iopub.execute_input":"2024-08-08T07:51:56.098867Z","iopub.status.idle":"2024-08-08T07:53:45.044012Z","shell.execute_reply.started":"2024-08-08T07:51:56.098820Z","shell.execute_reply":"2024-08-08T07:53:45.043066Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Evaluating the model before training!\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='172' max='172' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [172/172 01:47]\n    </div>\n    "},"metadata":{}},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.7840155959129333,\n 'eval_accuracy': 0.6116363636363636,\n 'eval_f1': 0.31185567010309273,\n 'eval_precision': 0.599009900990099,\n 'eval_recall': 0.21080139372822299,\n 'eval_runtime': 108.9325,\n 'eval_samples_per_second': 12.622,\n 'eval_steps_per_second': 1.579}"},"metadata":{}}]},{"cell_type":"code","source":"print(\"Training the model\")\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-08T07:59:11.150781Z","iopub.execute_input":"2024-08-08T07:59:11.151143Z","iopub.status.idle":"2024-08-08T08:53:54.531556Z","shell.execute_reply.started":"2024-08-08T07:59:11.151113Z","shell.execute_reply":"2024-08-08T08:53:54.530684Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Training the model\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2748' max='6870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2748/6870 54:41 < 1:22:05, 0.84 it/s, Epoch 2/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.598600</td>\n      <td>0.512927</td>\n      <td>0.821818</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.334000</td>\n      <td>0.801839</td>\n      <td>0.812364</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2748, training_loss=0.4880185938037639, metrics={'train_runtime': 3282.9595, 'train_samples_per_second': 8.37, 'train_steps_per_second': 2.093, 'total_flos': 4836396803328000.0, 'train_loss': 0.4880185938037639, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"code","source":"print(\"Evaluating the trained model\")\ntrainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-08-08T08:54:58.701269Z","iopub.execute_input":"2024-08-08T08:54:58.701945Z","iopub.status.idle":"2024-08-08T08:57:23.639193Z","shell.execute_reply.started":"2024-08-08T08:54:58.701911Z","shell.execute_reply":"2024-08-08T08:57:23.638291Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Evaluating the trained model\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='344' max='344' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [344/344 02:24]\n    </div>\n    "},"metadata":{}},{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.5117854475975037,\n 'eval_accuracy': 0.8247272727272728,\n 'eval_runtime': 144.9292,\n 'eval_samples_per_second': 9.487,\n 'eval_steps_per_second': 2.374,\n 'epoch': 2.0}"},"metadata":{}}]},{"cell_type":"code","source":"print(\"Saving the model\")\nlora_model.save_pretrained('fine-tuned-model')","metadata":{"execution":{"iopub.status.busy":"2024-08-08T08:57:23.641015Z","iopub.execute_input":"2024-08-08T08:57:23.641708Z","iopub.status.idle":"2024-08-08T08:57:24.351390Z","shell.execute_reply.started":"2024-08-08T08:57:23.641671Z","shell.execute_reply":"2024-08-08T08:57:24.350558Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Saving the model\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Making predictions","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\nclf = pipeline(\"text-classification\", lora_model, tokenizer = MODEL_ID)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.info()","metadata":{"execution":{"iopub.status.busy":"2024-08-08T09:04:57.711625Z","iopub.execute_input":"2024-08-08T09:04:57.712456Z","iopub.status.idle":"2024-08-08T09:04:57.722287Z","shell.execute_reply.started":"2024-08-08T09:04:57.712416Z","shell.execute_reply":"2024-08-08T09:04:57.721215Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3263 entries, 0 to 3262\nData columns (total 3 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   id             3263 non-null   int64 \n 1   combined_text  3263 non-null   object\n 2   label          3263 non-null   int64 \ndtypes: int64(2), object(1)\nmemory usage: 76.6+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"test_data = test_data[['id', 'combined_text']].dropna()","metadata":{"execution":{"iopub.status.busy":"2024-08-08T09:02:13.618071Z","iopub.execute_input":"2024-08-08T09:02:13.618986Z","iopub.status.idle":"2024-08-08T09:02:13.627678Z","shell.execute_reply.started":"2024-08-08T09:02:13.618936Z","shell.execute_reply":"2024-08-08T09:02:13.626623Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-08T09:07:07.706587Z","iopub.execute_input":"2024-08-08T09:07:07.707223Z","iopub.status.idle":"2024-08-08T09:07:07.719912Z","shell.execute_reply.started":"2024-08-08T09:07:07.707188Z","shell.execute_reply":"2024-08-08T09:07:07.718964Z"},"trusted":true},"execution_count":66,"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"   id                                      combined_text  label\n0   0           Text: just happened a terrible car crash      0\n1   2  Hashtags: earthquake. Text: heard about  is di...      0\n2   3  Text: there is a forest fire at spot pond gees...      0\n3   9  Hashtags: spokane, wildfires. Text: apocalypse...      0\n4  11  Text: typhoon soudelor kills 28 in china and t...      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>combined_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Text: just happened a terrible car crash</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Hashtags: earthquake. Text: heard about  is di...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Text: there is a forest fire at spot pond gees...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>Hashtags: spokane, wildfires. Text: apocalypse...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>Text: typhoon soudelor kills 28 in china and t...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_data['label'] = 0","metadata":{"execution":{"iopub.status.busy":"2024-08-08T09:04:55.543388Z","iopub.execute_input":"2024-08-08T09:04:55.543746Z","iopub.status.idle":"2024-08-08T09:04:55.548218Z","shell.execute_reply.started":"2024-08-08T09:04:55.543718Z","shell.execute_reply":"2024-08-08T09:04:55.547372Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\npredictions = []\n\nprint(\"Making prediction on test dataset...\")\n\nfor text in tqdm(test_data['combined_text'].values):\n    prediction = clf(text)\n    predicted_label = prediction[0]['label']\n    \n    #print(predicted_label)\n    \n    # Convert the string label to an integer using the label2id dictionary\n    prediction_int = label2id.get(predicted_label, None)  # Use None or a default value if label is missing\n\n    # Append the integer prediction to the list\n    predictions.append(prediction_int)\n    \n# Add the predictions to the DataFrame\n#test_data['label'] = predictions","metadata":{"execution":{"iopub.status.busy":"2024-08-08T09:16:46.341837Z","iopub.execute_input":"2024-08-08T09:16:46.342186Z","iopub.status.idle":"2024-08-08T09:29:32.332991Z","shell.execute_reply.started":"2024-08-08T09:16:46.342156Z","shell.execute_reply":"2024-08-08T09:29:32.332022Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"Making prediction on test dataset...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3263/3263 [12:45<00:00,  4.26it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n\nsample_submission['target'] = predictions\nsample_submission.to_csv(f'submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T09:31:38.761139Z","iopub.execute_input":"2024-08-08T09:31:38.761523Z","iopub.status.idle":"2024-08-08T09:31:38.777871Z","shell.execute_reply.started":"2024-08-08T09:31:38.761489Z","shell.execute_reply":"2024-08-08T09:31:38.777139Z"},"trusted":true},"execution_count":80,"outputs":[]}]}